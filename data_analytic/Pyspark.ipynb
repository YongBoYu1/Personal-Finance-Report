{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9txx35V6heK",
        "outputId": "c94f3073-3b0b-48f3-e8bf-9657066f613f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488490 sha256=2c3fedc41ae2df7071c81dda4342d7f2a0e4c642bf95586def84e7b3c88e1783\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wn1QlLJGZGPg",
        "outputId": "9a316563-7ed3-4d4c-dc0e-a9615ff10514"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"CoLabExample\") \\\n",
        "    .config(\"spark.driver.memory\", \"5g\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "B14mcAn1kkqR"
      },
      "outputs": [],
      "source": [
        "# Set the legacy time parser policy\n",
        "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "uxB11nGTGhac"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "from pyspark.sql.functions import when, col, \\\n",
        "to_date, date_format, regexp_replace, lit, min, max,count,  year, month, abs\n",
        "from functools import reduce\n",
        "from pyspark.sql.types import StringType, DateType\n",
        "from datetime import datetime, timedelta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RpL2nL1-iOTs"
      },
      "outputs": [],
      "source": [
        "def generate_date_range(from_date, to_date):\n",
        "    # Parse the input strings to datetime objects\n",
        "    from_date = datetime.strptime(from_date, '%Y-%m-%d')\n",
        "    to_date = datetime.strptime(to_date, '%Y-%m-%d')\n",
        "\n",
        "    # Calculate the number of days between the two dates\n",
        "    delta = (to_date - from_date).days\n",
        "\n",
        "    # Generate the date list\n",
        "    date_list = [(from_date + timedelta(days=i)).strftime('%Y-%m-%d') for i in range(delta + 1)]\n",
        "\n",
        "    return date_list\n",
        "\n",
        "\n",
        "def transform_df(df):\n",
        "    # Convert 'Date' column to date format\n",
        "    #df = df.withColumn('Date', to_date(col('Date'), 'MMM dd, yyyy'))\n",
        "\n",
        "    # Create 'withdraw' DataFrame\n",
        "    withdraw_df = df.select(col('Date'), col('Description'), col('Withdrawals').alias('Amount'))\\\n",
        "                    .withColumn('Type', lit('withdraw')).filter(col('Amount') != 0)\n",
        "\n",
        "    # Create 'deposits' DataFrame\n",
        "    deposits_df = df.select(col('Date'), col('Description'), col('Deposits').alias('Amount'))\\\n",
        "                    .withColumn('Type', lit('deposits')).filter(col('Amount') != 0)\n",
        "\n",
        "    # Union the two DataFrames\n",
        "    return withdraw_df.union(deposits_df).sort('Date')\n",
        "\n",
        "\n",
        "\n",
        "def format_finance_df(df, type='banking'):\n",
        "    \"\"\"_summary_\n",
        "\n",
        "    Args:\n",
        "        df (DataFrame): The DataFrame to format.\n",
        "        type (str, optional): Wheather is credit or banking account. Defaults to 'banking'.\n",
        "\n",
        "    Returns:\n",
        "        _type_: _description_\n",
        "    \"\"\"\n",
        "    res_df = df\n",
        "    if type == 'banking':\n",
        "       # Cast Date column in to yyyy-MM-dd format. eg: (Jul 22, 2023) -> (2023-07-22)\n",
        "       res_df = res_df.withColumn('Date', date_format(to_date('Date', 'MMM dd, yyyy'), 'yyyy-MM-dd'))\n",
        "\n",
        "       for column in ['Withdrawals', 'Deposits', 'Balance']:\n",
        "        # For the column in 'Withdrawals', 'Deposits', 'Balance'. Rm the '$' ,',' '' '-'. Cast it into float object.\n",
        "        res_df = res_df.withColumn(\n",
        "            column,\n",
        "            regexp_replace(\n",
        "                regexp_replace(\n",
        "                    regexp_replace(col(column), '−', '-'),\n",
        "                    ',', ''\n",
        "                ),\n",
        "                '\\$', ''\n",
        "            ).cast('float')\n",
        "        )\n",
        "\n",
        "    else: # credit\n",
        "        # Cast Date column in to yyyy-MM-dd format. eg: (Jul 22, 2023) -> (2023-07-22)\n",
        "        res_df = res_df.withColumn('Date', date_format(to_date('Date', 'MMM dd, yyyy'), 'yyyy-MM-dd'))\n",
        "\n",
        "        # For the Amount column. Rm the '$' ,',' '' '-'. Cast it into float object.\n",
        "        res_df = res_df.withColumn('Amount',\n",
        "                            regexp_replace(\n",
        "                                regexp_replace(\n",
        "                                    regexp_replace(col('Amount'), '−', '-'),\n",
        "                                    ',', ''\n",
        "                                      ),\n",
        "                                    '\\$', ''\n",
        "                                    ).cast('float')\n",
        "                            )\n",
        "\n",
        "    return res_df\n",
        "\n",
        "\n",
        "def summarize_nas(df):\n",
        "    \"\"\"\n",
        "    Summarizes the missing values in a DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    df (pd.DataFrame): The DataFrame to summarize.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: A summary DataFrame containing the count and percentage of missing values for each column.\n",
        "    \"\"\"\n",
        "    # Calculate the number of missing values per column\n",
        "    nas = df.isna().sum()\n",
        "\n",
        "    # Calculate the percentage of missing values per column\n",
        "    nas_percent = (nas / len(df)) * 100\n",
        "\n",
        "    # Create a summary DataFrame\n",
        "    summary_df = pd.DataFrame({\n",
        "        'Missing Values': nas,\n",
        "        'Percentage': nas_percent\n",
        "    }).sort_values(by='Missing Values', ascending=False)\n",
        "\n",
        "    return summary_df\n",
        "\n",
        "\n",
        "def spark_df_basic_stats(df):\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  # Print schema\n",
        "  print(\"Schema:\")\n",
        "  df.printSchema()\n",
        "\n",
        "  # Count rows\n",
        "  row_count = df.count()\n",
        "  print(f\"Row count: {row_count}\")\n",
        "\n",
        "  # Summary statistics for numerical columns\n",
        "  print(\"Summary statistics:\")\n",
        "  df.describe().show()\n",
        "\n",
        "  # Non-null count for each column\n",
        "  print(\"Non-null counts for each column:\")\n",
        "  non_null_counts = df.select([count(col(c)).alias(c) for c in df.columns]).collect()[0].asDict()\n",
        "  for column, non_null_count in non_null_counts.items():\n",
        "      print(f\"{column}: {non_null_count}\")\n",
        "\n",
        "  # Show first few rows\n",
        "  print(\"First few rows:\")\n",
        "  df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVRBYg3SBEsj"
      },
      "source": [
        "The Debit tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DHbfmjYJZ0vf"
      },
      "outputs": [],
      "source": [
        "# TD data\n",
        "td_cheq = spark.read.csv('../content/drive/My Drive/personal_fin_data/TD_EVERY_DAY_CHEQUING_ACCOUNT.csv',\n",
        "                         header=True,\n",
        "                         inferSchema=True)\n",
        "\n",
        "td_save = spark.read.csv('../content/drive/My Drive/personal_fin_data/TD_EVERY_DAY_SAVINGS_ACCOUNT.csv',\n",
        "                         header=True,\n",
        "                         inferSchema=True)\n",
        "\n",
        "td_cheq = format_finance_df(td_cheq)\n",
        "td_save = format_finance_df(td_save)\n",
        "\n",
        "\n",
        "# RBC Data\n",
        "rbc_cheq = spark.read.csv('../content/drive/My Drive/personal_fin_data/RBC_Day_to_Day_Banking.csv',\n",
        "                         header=True,\n",
        "                         inferSchema=True)\n",
        "\n",
        "\n",
        "# For the rbc data. Fix the sign\n",
        "rbc_cheq = rbc_cheq.withColumn('Withdrawals', col('Withdrawals')*-1)\n",
        "rbc_cheq = format_finance_df(rbc_cheq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KrCkhms9MP5d"
      },
      "outputs": [],
      "source": [
        "# Transform all the debit df\n",
        "debit_dfs = [td_cheq, td_save, rbc_cheq]\n",
        "trans_debit_dfs = [transform_df(df) for df in debit_dfs]\n",
        "\n",
        "# Merge all the debit data.\n",
        "debit_df = reduce(lambda df1, df2: df1.union(df2), trans_debit_dfs)\n",
        "\n",
        "# Sort by Date in descending order\n",
        "debit_df = debit_df.orderBy(col('Date').desc())\n",
        "\n",
        "debit_df = debit_df.withColumn('Category', lit(None))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7bs69-j_8j6"
      },
      "source": [
        "The credit table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-FSywWIgXxy0"
      },
      "outputs": [],
      "source": [
        "cibc_credit = spark.read.csv('../content/drive/My Drive/personal_fin_data/CIBC_MasterCard_20240714_to_20240418.csv',\n",
        "                         header=True,\n",
        "                         inferSchema=True)\n",
        "\n",
        "cibc_credit = format_finance_df(cibc_credit, 'credit')\n",
        "\n",
        "# Create the 'Type' column in df fill with value 'credit'\n",
        "cibc_credit = cibc_credit.withColumn('Type', lit('credit'))\n",
        "\n",
        "# rename Merchant into Description\n",
        "cibc_credit = cibc_credit.withColumnRenamed('Merchant', 'Description')\n",
        "\n",
        "cibc_credit = cibc_credit.select(\n",
        "    col('Date'),\n",
        "    col('Description'),\n",
        "    col('Amount'),\n",
        "    col('Type'),\n",
        "    col('Category'),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "kwsV-qIwlAR7"
      },
      "outputs": [],
      "source": [
        "# Stack (concatenate) the DataFrames\n",
        "stacked_df = debit_df.unionByName(cibc_credit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "UeHPddt_lEaD"
      },
      "outputs": [],
      "source": [
        "stacked_df = stacked_df.orderBy(col('Date'))\n",
        "\n",
        "stacked_df = stacked_df.fillna({'Category':'Debit_Card'})\n",
        "#stacked_df = stacked_df.withColumn('Amount', when(col('Type') == 'withdraw', col('Amount') * -1).otherwise(col('Amount')))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Ygx4Kb6RAgcd"
      },
      "outputs": [],
      "source": [
        "# Create a continuely daily range df\n",
        "max_date , min_date  = stacked_df.select(max('Date')).collect()[0][0],stacked_df.select(min('Date')).collect()[0][0]\n",
        "\n",
        "\n",
        "date_range = generate_date_range(min_date, max_date)\n",
        "\n",
        "date_df = spark.createDataFrame(date_range, StringType()).toDF('Date')\n",
        "\n",
        "# Rename 'Date' column in date_df to avoid ambiguity\n",
        "date_df = date_df.withColumnRenamed('Date', 'Date_temp')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hkpB4PxEeT5",
        "outputId": "46e8a5da-76aa-49bf-9c3b-dd2ec12b37ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['deposits', 'withdraw', 'credit']\n"
          ]
        }
      ],
      "source": [
        "# Get all unique values from the 'Type' column\n",
        "unique_types = stacked_df.select(col('Type')).distinct().collect()\n",
        "\n",
        "# Extract and print the unique values\n",
        "unique_types_list = [row['Type'] for row in unique_types]\n",
        "print(unique_types_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eABiwRKkGvX_",
        "outputId": "32a6a731-a0c8-465b-cc24-7e03433075ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Date: string (nullable = true)\n",
            " |-- Description: string (nullable = false)\n",
            " |-- Amount: float (nullable = false)\n",
            " |-- Type: string (nullable = false)\n",
            " |-- Category: string (nullable = false)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "full_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ealvm37sNtMt"
      },
      "outputs": [],
      "source": [
        "# merge date df\n",
        "# Perform a left join to fill missing dates\n",
        "full_df = date_df.join(stacked_df, date_df['Date_temp'] == stacked_df['Date'], 'left') \\\n",
        "    .fillna({'Amount': 0, 'Description': 'No Transaction', 'Type': 'None', 'Category': 'None'}) \\\n",
        "    .drop('Date') \\\n",
        "    .withColumnRenamed('Date_temp', 'Date')\n",
        "\n",
        "\n",
        "res_df = full_df.withColumn('Processed_Amount',\n",
        "    when(col('Type') == 'withdraw', -abs(col('Amount')))\n",
        "    .when(col('Type') == 'deposit', abs(col('Amount')))\n",
        "    .when((col('Type') == 'credit') & (col('Amount') > 0), -col('Amount'))\n",
        "    .when((col('Type') == 'credit') & (col('Amount') <= 0), 0)\n",
        "    .otherwise(col('Amount'))\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMnGdcyBPu89"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "qT08a3oClN_y"
      },
      "outputs": [],
      "source": [
        "# Convert PySpark DataFrame to Pandas DataFrame\n",
        "pandas_df = res_df.toPandas()\n",
        "\n",
        "# Step 3: Save the CSV file to the local filesystem of Colab\n",
        "csv_filename = '/content/transactions.csv'\n",
        "pandas_df.to_csv(csv_filename, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iliSj0D0sOD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koUwhAxJBHY_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
